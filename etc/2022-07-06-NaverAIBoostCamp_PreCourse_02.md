---
layout: post
title: "네이버AI 부스트캠프 Pre-course 02. 벡터와 행렬"
subtitle: "AI수학: 벡터와 행렬"
date: 2022-07-06
author: "Lee Je Hee"
URL: "/2022/07/06/NaverAIBoostCamp_PreCourse_02/"
published: False
tags:
 - AI
 - MachineLearning
 - Naver
 - Math
---
# 벡터란?
벡터는 수학에서 '크기와 방향을 갖춘 양'이라고 설명된다.

벡터를 프로그래밍에서 쉽게 설명하자면 숫자들로 이루어진 리스트(list)를 말한다.

또한 벡터를 python의 numpy 모듈에서 표현할 때는 배열(array)로 표현한다.

```python
x = [1,2,3]

x = np.array([1,2,3])
```

## 열벡터
열벡터는 '세로'의 형태를 가진다.
![[img/2022_07/1.png|200]]


## 행벡터
행벡터는 '가로'의 형태를 가진다.
![[img/2022_07/2.png|400]]

위 그림의 경우 $X$ 위에 $^T$ 가 그려져 $X^T$ 로 표현되고 있다.

이는 행벡터가 열벡터 $X$의 전위행렬(transpose) 모습이기 때문이다.

또한 위 그림의 경우 $X$가 총 $d$ 개 만큼 있다는 사실을 알 수 있다.

$d$는 '벡터의 차원'을 의미한다.

## 공간 상에서 벡터
공간에서 벡터는 <b><u>한 점</u></b>을 나타낸다.

공간에서 벡터는 공간 내 원점을 기준으로한 상대적인 위치를 의미하며,

원점에서 한 점으로 향하는 <b><u>화살표</u></b>로 표현된다.

![[4.png|300]]
2차원에서의 벡터 표현

## 스칼라곱
'벡터에 숫자를 곱해주는 경우'를 **스칼라곱**이라고 한다.
![[img/2022_07/7.png|300]]


공간 상에서의 벡터 표현법에서 '벡터에 숫자를 곱해주는 경우' 위의 그림과 같이 화살표의 길이가 변한다.

이때 곱해주는 숫자 $α$ 의 범위에 따라 화살표의 길이, 방향이 아래와 같이 바뀐다.

$α<0$: 벡터가 반대 방향으로

$0<α<1$: 벡터가 짧아짐

$1<α$: 벡터가 길어짐
	
![[ img/2022_07/10.png|500]]




# 덧셈, 뺄셈, 곱셈

두 벡터의 모양이 같을 경우 덧셈과 뺄셈이 가능하다.
![[이미지 105.png]]
## 덧셈
공간 상에서 덧셈은 원점에서 한 벡터로 이동한 다음, 다시 다른 벡터로 이동하는 것과 같이 표현된다.

공간 상에서 덧셈은 아래의 그림과 같다.
![[이미지 106.png|300]]

## 뺄셈
공간 상에서 뺄셈은 반대방향으로 향하는 벡터를 덧셈하는 것과 같다.

공간 상에서의 뺄셈은 아래의 그림과 같다.
![[이미지 107.png|300]]

## 성분곱
두 벡터의 모양이 같을 경우 성분곱이 가능하다.![[이미지 104.png]]

성분곱은 동일한 배열 위치의 성분끼리 곱하는 것을 말한다.

성분곱은 코드를 통해 간단하게 표현할 수 있다.

```python
import numpy as np

x = np.array([1,2,3])
y = np.array([2,3,4])

print (x * y)
```

# 노름(norm)
노름은 '원점을 기준으로 한 거리'를 의미한다.

노름의 기호는 $||X||$와 같은 기호로 표시된다.

노름의 종류는 두 가지로 나뉜다.
![[이미지 103.png]]
## $L1$
$L1$ 노름은 각 성분의 변화량의 절댓값을 더한다.

$L1$ 노름은 코드를 통하여 다음과 같이 구현될 수 있다.

```python
def L1_norm(x):
	x_norm = np.abs(x)    #abs는 절대값을 말함
	x_norm = np.sum(x_norm)
	
	return x_norm
```

## $L2$ 
$L2$ 노름은 피타고라스의 정리를 이용하여 유클리드 거리를 구한다.

$L2$ 노름은 코드를 통하여 다음과 같이 구현될 수 있다.

```python
def L2_norm(x):
	x_norm = x*x
	x_norm = np.sum(x_norm)
	x_norm = np.sqrt(x_norm)   #sqrt는 square root를 말함

	return x_norm
```
혹은 numpy 모듈의 `np.linalg.norm`을 사용해도 된다.

## L1과 L2의 차이, 나뉘는 이유
두 노름은 기하학적으로 차이를 지닌다.
![[11.PNG]]
이 차이로 인하여 인공지능 알고리즘에서도 서로 다른 용도로 사용된다.

$L1$ 노름: Robust 학습, Lasso 회귀 등...
$L2$ 노름: Laplace 근사, Ridge 회귀 등...

# 서로 다른 벡터 비교
## 벡터 사이의 거리
두 벡터 사이의 거리는 어떻게 구할 수 있을까?

![[이미지 101.png|250]]
두 벡터 사이의 거리는 뺄셈을 이용하여 계산한다.

![[이미지 102.png|250]]
$||X-Y|| = ||Y-X||$이기 때문에, $'Y'$와 $'-X'$ 사이의 거리를 구하는 것과 같다.


## 벡터 사이의 각도
두 벡터 사이의 각도는 '제2코사인법칙'을 활용한다.

이때, 분자의 수식이 **내적**(inner product)의 형태를 가진다.

## 내적(inner product)
내적은 정사영(orthogonal projection)된 벡터의 길이와 관련이 있다.

Proj($X$)는 벡터 $Y$로 정사영된 벡터 $X$의 그림자를 의미한다.

Proj($X$)의 길이는 코사인법칙에 의해 $||X||cosθ$가 된다.

내적(inner product)는 정사영의 길이 $||X||cosθ$를 벡터 $Y$의 길이만큼 조정한 것이다.

따라서 내적은 