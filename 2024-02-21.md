# Week2: Sum of Expectations
- 기댓값끼리는 더할 수 있다.
$$\huge \mathbb E[X_1]+ \mathbb E[X_2]=\mathbb E[X_1+X_2]$$
- 확률분포의 종류, 사건의 독립/종속 여부와 상관없이 '항상' 더할 수 있다.
# Week2: Variance
분산.
$$\huge\begin{align}
Var(X)&=\mathbb E[\ (X-\mathbb E[X])^2\ ]\\
\\
&=\mathbb E[X^2]-\mathbb E[X]^2 
\end{align}$$
- 편차(관측값-기댓값)의 제곱의 평균
- 따라서 기댓값 - 편차 - 편차제곱 - 이에대한 평균을 구하는 방식으로 계산하면 된다.'
- 분산은 가중평균(weighted average)이다.
### 분산은 왜 필요하게 됐을까?
- 기댓값만으로는 확률분포를 전부 설명할 수 없다.
	- 예를 들어 [-1, 1] 과 [-100, 100] 데이터 모두 기댓값은 0이다.
- 편차(관측값-기댓값)만으로 설명하자니, 양수와 음수 때문에 설명이 쉽지 않다.
- 이에 편차를 제곱하고 이를 다시 평균내서 확률분포를 비교하게 되었다.
### 기댓값의 몇가지 성질
- $\huge \mathbb E[c \cdot X]=c \cdot \mathbb E[X]$
- $\huge\mathbb E[X] = \text{상수}$
- $\huge\mathbb E[c]=c$
	- $\huge \mathbb E[\mathbb E[X]]=\mathbb E[X]$
### 분산의 성질
- $\huge Var(aX+b)=a^2Var(X)$
# Week2: Standard Deviation
표준편차.
$$\huge std(X)=\sqrt{Var(X)}$$
- 분산을 통해 데이터를 설명하려다보니 단위가 항상 원래 데이터의 제곱이 되어버린다.
	- 키에 대한 데이터의 분산의 단위는 미터제곱이니, 이는 면적에 대한 단위가 아닌가
- 데이터를 더 잘 설명하기 위해 분산의 제곱근을 표준편차라고 칭한다.
### Normal Distribution: 68-95-99.7 rule
<img src="https://www.freecodecamp.org/news/content/images/2020/08/normal_dist_68_rule_heights.jpg">
- 정규분포를 따르는 데이터가 있다고 하자.
	- $\huge X\sim N(\mu, \sigma^2)$
	- $\mu\text{: 기댓값}$
	- $\sigma\text{: 편차}$
- 이 데이터의 확률밀도함수의 면적은 다음을 따른다.
	- $\huge (\mu-\sigma) \sim (\mu+\sigma): 68\%$
	- $\huge (\mu-2\sigma) \sim (\mu+2\sigma): 95\%$
	- $\huge (\mu-3\sigma) \sim (\mu+3\sigma): 99.7\%$
# Week2: Sum of Gaussians
- 서로 독립인 정규분포(가우시안 분포)끼리는 기댓값과 편차를 단순하게 더한 정규분포로 표현이 가능하다.
$$\huge\begin{align}
&\begin{cases} X\sim N(\mu\small_X\huge,\sigma^2\small_X\huge)\\
			  Y\sim N(\mu\small_Y\huge,\sigma^2\small_Y\huge)& \small\text{X, Y independent} \end{cases}\\
\\
&W=aX+bY\\
&\rightarrow W\sim N(a\mu_x+b\mu_y,\ a^2\sigma^2_x+b^2\sigma^2_y)

\end{align}$$
- 서로 독립된 확률$분포가 둘 있다고 하자.
	- $T\sim N(10, 2^2)$
	- $L\sim N(5,1^2)$
- 이 둘을 더한 확률분포는 다음과 같다.
$$\begin{align}
&R=T+L\\
&R\sim N(10+5, 4+1)
\end{align}$$
# Week2: Standardizing a Distribution
$$\huge X \rightarrow \frac{X-\mu}{\sigma}$$
### 1. 기댓값($\mu$)은 0인 것이 좋다.
- 통계분석에서 $\mathbb E[X]$말고 $\mathbb E[X-\mu]$를 분석하는게 더 편할 때가 있다.
	- $\mathbb E[X-\mu]=0$이기 때문이다.
### 2. 표준편차($\sigma$)는 1인 것이 좋다.
- 확률분포에 변형이 가해질 때 계산이 쉬워지기 때문이다.
### Standardizing a distribution has several benefits.
Firstly, it transforms datasets into a standard scale, making it easier to compare between different datasets.
Secondly, it simplifies statistical analysis, particularly when using techniques that assume a standard normal distribution.
Finally, standardizing features in machine learning can improve the convergence rate of optimization algorithms and prevent some features from dominating others, leading to improved model performance.
# Week2: Skewness and Kurtosis: Moments of a Distribution
## Moment
적률. 모멘트.
$$\huge \mathbb E[X^k]=p_1x_1^k+p_2x_2^k+\cdots+p_nx_n^k$$
- 확률분포의 변수를 일정한 차수의 거듭제곱한 값에 대한 평균.
### 참고
- https://blog.naver.com/mykepzzang/220846464280
# Week2: Skewness and Kurtosis - Skewness
### Skewness
왜도. 비대칭도.
$$\huge\mathbb E[(\frac{X-\mu}{\sigma})^3\ ]$$
- 세번째 모멘트를 정규화한 형태이다.
### Positively/Negatively Skewed
<img src="https://qph.cf2.quoracdn.net/main-qimg-1b2514ed201474a7895bd20de8445cae-lq">

- 데이터가 오른쪽으로 치우치면 positively skewed라고 표현한다.
	$\huge\mathbb E[(\frac{X-\mu}{\sigma})^3\ ]>0$
- 데이터가 어디에도 치우치지 않으면 왜도가 없다고 표현한다.
	$\huge\mathbb E[(\frac{X-\mu}{\sigma})^3\ ]=0$
- 데이터가 왼쪽으로 치우치면 negatively skewed라고 표현한다.
	$\huge\mathbb E[(\frac{X-\mu}{\sigma})^3\ ]<0$
### 참고
- https://www.quora.com/What-is-positive-skewness-indicating-I-am-not-asking-about-symmetry-just-an-application
### Skewness는 왜 필요할까?
- 기댓값, 분산만으로도 비교하기 힘든 데이터들이 있다.
	- 예를들어 1%확률로 99달러를 얻거나 99%확률로 1달러를 얻는 로또게임과, 99%확률로 1달러를 얻거나 1%확률로 99달러를 잃는 보험게임이 있다고 하자.
	- 두 게임은 같은 기댓값(0)과 같은 분산(99)을 가지지만, 정반대의 게임이다.
	- 이를 설명하기 위해서 moment 개념에서 비롯한 skewness 개념을 사용한다.
# Week2: Skewness and Kurtosis - Kurtosis
### Kurtosis
첨도.
$$\huge\mathbb E[(\frac{X-\mu}{\sigma})^4\ ]$$
- 네번째 모멘트를 정규화한 형태이다.
### Mesokurtic/Leptokurtic/Platykurtic
<img src="https://i2.wp.com/www.tutorialspoint.com/statistics/images/kurtosis.jpg?zoom=1.75&w=578&ssl=1">
- 첨도는 그래프 양끝의 꼬리를 보면 된다.
- 꼬리가 두꺼우면 Platykurtic
- 꼬리가 평균이면 Mesokurtic
- 꼬리가 얇으면 Leptokurtic
### 참고
- https://www.r-bloggers.com/2020/11/skewness-and-kurtosis-in-statistics/
### Kurtosis는 왜 필요한가?
- 기댓값, 분산, 그리고 왜도로도 비교하기 힘든 데이터들이 있다.
	- 앞면이 나오면 1달러를 얻고, 뒷면이 나오면 1달러를 잃는 동전게임과, $\frac{100}{202}$ 확률로 0.1달러를 얻거나 잃고, $\frac{1}{202}$ 확률로 10달러를 얻거나 잃는 카드게임이 있다고 하자.
	- 두 게임은 같은 기댓값(0), 같은 분산(1), 그리고 같은 왜도(0)를 가지지만, 큰 차이를 가지는 게임이다.
	- 이를 설명하기 위해서 moment 개념에서 비롯한 kurtosis 개념을 사용한다.