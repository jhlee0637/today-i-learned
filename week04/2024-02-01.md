# Numpy
### np.random.seed(n)
# Perceptron
### Neural Network Model with a Single Perceptron and One Input Node
<img src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*upfpVueoUuKPkyX3PR3KBg.png">
- 간단한 형태의 neural network model은 perceptron을 이용해서 표현할 수 있다.
- Input layer는 node가 하나이다.
- Ouput layer도 node가 하나이다.
- Weight과 Bias가 업데이트되며 모델을 훈련(train)시킨다.
- $\huge i$번째 훈련은 다음과 같이 표현된다.
$$\huge \begin{align}
&z^{(i)} = wx^{i}+b\\
&\hat{y}^{(i)}=z^{(i)}\\
&\small i=1,\ \dots,\ m
\end{align}$$
### Forward propagation
순전파.
- 모델의 input layer부터 output layer까지 순서대로 변수들을 계산하고 저장
- 모든 m개의 훈련 데이터들을 모은 벡터 $\huge X\small(1\times m)$와 동일한 사이즈로 broadcasting된 $\huge Z$와 $\huge \hat{Y}$ 
$$\huge
\begin{align}
&Z=wX+b\\
&\hat{Y}=Z
\end{align}
$$
### cost function
-  $\huge i$번째 훈련마다 얻게되는 결과값 $\huge \hat{y}^{(i)}$를 이용해서 손실함수 $\huge L(w,b)=\frac{1}{2}(\hat{y}^{(i)}-y^{(i)})^2$를 계산할 수 있다.
	- $\frac{1}{2}$ is taken just for scaling purposes, you will see the reason below, calculating partial derivatives
- 이 손실함수를 $\huge i, \dots, m$까지 모으면 두 벡터 $\huge \hat{Y} \small (1 \times m)$과 $\huge Y \small (1 \times m)$를 비교할 수 있게 된다.
$$\huge \mathcal{L}(w,b)=\frac{1}{2m}\sum^{m}_{i=1}(\hat{y}^{(i)}-y^{(i)})^2$$
### Backward propagation
역전파
- weight과 bias를 조정해서 cost function을 낮춘다.
### 참고
- https://towardsdatascience.com/whats-the-role-of-weights-and-bias-in-a-neural-network-4cf7e9888a0f