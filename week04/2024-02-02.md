# General Methodology to build a neural network
1. Define the nerual network structures
	- input units, hidden units, ... 
2. Initialize the model's parameters
3. LOOP
	- Implement forward propagation
	- Implement backward propagation
	- Update parameters
4. Make predictions
# Single perceptron neural network with sigmoid activation function
$$\huge
\begin{align}
&z^{(i)} = Wx^{(i)} + b,\\
&a^{(i)} = \sigma(z^{(i)})
\end{align}$$
### Classification Problem
- Regression problem은 문제에 대한 해를 구해야하지만, classification problem은 문제에 대한 T/F(혹은 0/1)을 구해야 한다.
- 따라서 문제에 대한 해를 먼저 구한 다음, 이 해가 T/F 중 어느 쪽에 해당하는지 판단하는 추가적인 과정이 필요하며, 이는 activation function을 통해서 구현된다.
### Activation Function
- 먼저 input, weight, bias를 통해 해 $\huge z$를 구한다.
	- 이 값은 다시 weight 벡터 $\huge W\small=[w_1\ w_2]$과 $\huge x^{(i)}\small=[x_1^{(i)}\ x_2^{(i)}]$로 표현된다.
$$\huge
\begin{align}
z^{(i)} &= w_1x_1^{(i)} + w_2x_2^{(i)} \\
        &= Wx^{(i)}+b
\end{align}$$
- 구한 해 $\huge z$를 activation function에 대입해서 0에서 1 사이의 값으로 낼 것이다.
	- Activation function은 sigmoid 기호($\huge \sigma$)로 나타내기 때문에 sigmoid function이라고도 불린다.
	$$\huge a=\sigma(z) = \frac{1}{1+e^{-z}}$$
	- $\huge a$값이 0.5 이상이면 $\huge 1$, 0.5 이하이면 $\huge 0$을 내도록 threshold를 설정하면 된다.
### log loss (in $m$ dimension)
- $(2 \times m)$개의 훈련데이터를 갖고있다고 하자. ($x_1, \dots, x_m$)
- 이 훈련 데이터를 activation function에 적용하고 싶다.
	- 그렇다면 모델은 다음과 같은 벡터들로 표현될 수 있다.
	$$\huge \begin{align}
&Z=WX+b\\
&A=\sigma(Z)\\
\end{align}$$
	- $\huge b$는 이제 broadcasting을 통해 $\huge (1\times m)$ 사이즈가 된다. 
- Classification 문제에 대한 loss function은 log loss로 나타낸다.
		$$\huge
	\begin{align}
	\mathcal{L}(W,b) &=\frac{1}{m}\sum^{m}_{i=1}L(W,b)\\
	&=\frac{1}{m}\sum^{m}_{i=1}(-y^{(i)}\log(a^{(i)}-(1-y^{(i)})\log(1-a^{(i)}))
	\end{align}$$
- Loss function의 최적값을 만들어내는 weight과 bias를 찾는게 목적인 것은 변함이 없다.
- Loss function에 대한 편미분과 chain rule을 이용하여 최적값을 찾아낼 수 있다.
$$\huge
\begin{align}
&&W=&W-\alpha\frac{\partial \mathcal{L}}{\partial W}\\
&&b=&b-\alpha\frac{\partial \mathcal{L}}{\partial b}
\end{align}$$
# Numpy
### np.logical_and()
### np.exp()