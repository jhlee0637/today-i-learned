# Parial derivatives
편도함수.
- 두 개의 미지수로 이루어진 함수를 미분하려고 할 때, 하나의 미지수는 상수(constant)로 취급하고 다른 미지수를 대상으로 하여 미분한다.
$$\huge
\begin{align}
f(x, y) & \rightarrow & f_x = \frac{\partial f}{\partial x}\\
		&             & f_y = \frac{\partial f}{\partial y}
\end{align}
$$
### Gradients of partial derivatives
- 편도함수 두 개를 성분으로 갖는 벡터(vector)를 만들고, 이를 gradient라고 부른다.
$$\huge \nabla f = \begin{bmatrix}
					\frac{\partial f}{\partial x}\\
					\frac{\partial f}{\partial y}
				   \end{bmatrix}$$
- 이 벡터는 nabla letter($\huge \nabla$)를 사용해서 표현한다.
### Optimization in Two variables
- 두 개의 미지수로 이루어진 함수의 최적값(최대 혹은 최소)는 어떻게 찾을 수 있을까?
- $\huge f_x = \frac{\partial f}{\partial x}$와 $\huge f_y = \frac{\partial f}{\partial y}$가 $\huge 0$이 되는 $\huge x$와 $\huge y$의 값을 찾아내서, 그 값을 $\huge f(x, y)$에 대입하면 최적값을 찾을 수 있다.
# Linear Regression
선형회귀.
- 좌표평면 위 세 개의 점이 있다.
- 이 세 개의 점과 가장 가깝게(최적으로) 위치할 수 있는 직선의 방정식을 알고싶다.
- 직선의 방정식은 $\huge y=mx+b$일 것이며, 미지수 $\huge m$과 $\huge b$의 최적값을 구하고 싶다.
- 이 직선의 방정식을 구하는 것이 선형회귀이다.
	- 이를 구하기 위해서는 각 점과 직선의 방정식 사이의 관계를 구해놓아야한다.
	- $\huge E(m,b)=\dots$와 같은 식이 나오게 된다.
	- 이 식의 최적값을 만드는 $\huge m$과 $\huge b$를 편도함수를 통해 알아낼 수 있다.
# Gradient Descent
경사하강법.
- 함수의 최적값을 구하는 방법은 도함수의 값이 $0$이 되는 최적점을 찾는 것이다.
- 그러나 계산식이 복잡하거나 정확한 해를 구하기 어려운 경우가 존재한다. (ex. $f(x)=e^{x}-log(x)$)
- 'Gradient가 $0$이 되는 정확한 해를 구할 수 없다면 거기에 가까운 해만 구해도 충분하지 않을까?'
	- 예를 들어 아래로 볼록한 함수가 있다.
	- 함수 위의 임의의 점을 하나 찍는다.
	- 우리의 목표는 기울기가 $0$이 되는 '최솟점'을 찾는 것이므로, 이 점의 기울기를 확인해본다.
		- 만약 기울기가 양수라면 아마 최솟점에서 왼쪽에 위치한 것이다.
			- 오른쪽으로 조금 이동해본다.
		- 만약 기울기가 음수라면 아마 최솟점에서 오른쪽에 위치한 것이다.
			- 왼쪽으로 조금 이동해본다.
	- 이렇게 점에서의 기울기를 확인하고 다음 점으로 계속 이동하다보면 최적점에 가까워질 것이다.
- 정확한 해에 가까운 해를 빠르게 구하는 방법이 경사하강법이다.
$$\huge
\begin{align}
&new\ point =  old\ point  - slope\\
& x_{1}    =  x_{0} - \alpha \ f'(x_{0})
	\end{align}$$
- 함수 $\huge f(x)$에 대해서 최솟값을 구하는 순서는
	1. Learning rate $\huge \alpha$를 정의한다
	2. $\huge x_{k}=x_{k-1}-\alpha f'(x_{k-1})$의 값을 업데이트 한다.
	3. 정확한 해에 가까워질 때까지 2번 항목을 반복한다. 
- 경사하강법은 정확한 해가 아니라 거기에 가까운 해를 구하는 것이다.
### Learning Rate가 중요한 이유
- 만약 Learning Rate가 너무 크면 해에 가까워지지 못하고 계속 왔다갔다만 하게된다
- 만약 Learning Rate가 너무 작으면 시간이 너무 오래 걸린다.