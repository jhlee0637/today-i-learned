# Complement Rule
$$\huge
P(A')=1-P(A)
$$
# Sum of Probabilities
### Disjoint Events
$$\huge P(A\cup B)=P(A)+P(B)$$
### Joint Events
$$\huge P(A\cup B)=P(A)+P(B)-P(A\cap B)$$
# Independent Events
독립사건.
- 앞의 사건이 뒤의 사건에 영향을 끼치지 않는다.
- 예시: 주사위를 반복해서 던짐
# Contional Probability
조건부 확률.
- 앞의 사건이 일어났다는 조건 하에 뒤의 사건의 확률은?
$$\huge\begin{align}P(A|B)\small:&\small\text{ B사건이 일어났다는 전제 하에 A사건의 확률}\\
&\small \text{(A given B)}\\
\end{align}$$
$$\huge P(A|B)=\frac{P(A\cap B)}{P(B)}$$
# General Product Rule
$$\huge P(A\cap B)=P(A)\cdot P(B|A)$$
$$\begin{align}
&\small \text{if)}&&\text{A and B are independent}\\
\\
&&&\huge P(B|A)=P(B)\\
&&&\huge \therefore P(A\cap B)=P(A)\cdot P(B)
\end{align}$$
# Bayes Theorem
$$\huge P(A|B)=\frac{P(A)\cdot P(B|A)}{P(A)\cdot P(B|A)+P(A')\cdot P(B|A')}$$
- 베이즈정리는 조건부확률 공식으로부터 시작한다.
### 질병검사 시나리오
- 다음의 시나리오를 배경으로 생각해보자.
	- 1,000,000명의 인구
	- 99% 정확도의 진단검사(100명의 환자 중 1명은 위음성이며, 100명의 정상인 중 1명은 위양성이다.)
	- 1/10,000의 인구가 질병에 걸려있음
	- 정확하게 양성으로 진단할 확률은?
- 이 시나리오를 토대로 확률을 작성하면 다음과 같다.
	- $P(\text{sick})=0.01\%$
	- $P(\text{not sick})=99.99\%$
	- $P(\text{dig. sick|sick})=99\%$
	- $P(\text{dig. sick|not sick})=1\%$
- 우리가 궁금한 조건부 확률은 다음과 같다.
	- $P(\text{sick|dig. sick})=?$
- 이 조건부확률 식을 전개한다.
$$\huge
\begin{align}
P(\text{sick|dig.sick})&=\frac{P(\text{sick}\cap\text{dig. sick})}{P(\text{dig. sick})}\\
\\
&\small P(\text{sick}\cap\text{dig. sick}) = P(\text{sick})\cdot P(\text{dig. sick}|\text{sick})\\
&\small P(\text{dig. sick})=P(\text{sick}\cap\text{dig. sick})+P(\text{not sick}\cap\text{dig. sick})\\
\\
&=\frac{P(\text{sick})\cdot P(\text{dig. sick}|\text{sick})}{P(\text{sick})\cdot P(\text{dig. sick}|\text{sick})+P(\text{not sick})\cdot P(\text{dig. sick}|\text{not sick)}}\\

\end{align}$$
# Stopwords
- In text, usually there are some words that don't provide much information about what the text is saying, such as prepositions, pronouns and so on. These are called **stopwords**.
- Since they are very common in every text, they hardly will store any meaningful information for our task. The idea is to remove all these stopwords and punctuation, so in the end you will have a simpler set of words to deal with. This is what the next function will do.
# Tokenization
- Another step is the emails **tokenization**. To tokenize is to split the email into **tokens**, which are essentially the words in it. As a result, for each email, the final result will be a numpy array consisting of every word in the email without stopwords and punctuation.
# naive assumption
# Machine Learning - Handling 0 in the Product
- 예를 들어, 특정 단어가 스팸 이메일에서 전혀 나타나지 않는 경우 해당 단어에 대한 스팸 클래스에서의 조건부 확률인 P(word|spam)은 0이 됩니다. 이는 해당 단어가 포함된 이메일이 스팸인 가능성이 전혀 없음을 의미합니다. 마찬가지로, 특정 단어가 스팸 이메일에서만 나타나고 햄 이메일에서는 전혀 나타나지 않는 경우, 해당 단어에 대한 햄 클래스에서의 조건부 확률은 0이 됩니다.
- 이러한 경우, 이러한 단어가 포함된 이메일이 특정 클래스에 속할 확률이 전체 확률을 0으로 만들 수 있습니다. 이는 모델이 매우 편향되고 부정확하게 되는 원인이 될 수 있습니다.
- 따라서 이러한 문제를 해결하기 위해, 모든 단어에 대해 스팸 및 햄 클래스에서 적어도 하나의 등장을 가정하는 것이 제안됩니다. 이렇게 하면 모든 단어에 대해 0이 아닌 스팸 및 햄 클래스에서의 빈도를 보장할 수 있으므로, 조건부 확률이 0이 되는 문제를 방지할 수 있습니다.